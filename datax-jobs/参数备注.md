## DataX
### json配置文件模板

1. 整个配置文件是一个job的描述；
2. job下面有两个配置项，content和setting，其中content用来描述该任务的源和目的端的信息，setting用来描述任务本身的信息；
3. content又分为两部分，reader和writer，分别用来描述源端和目的端的信息；
4. setting中的speed项表示同时起几个并发去跑该任务。

```json
{
  "job": {
    "content": {
      "reader": {
        
      },
      "writer": {
        
      }
    },
    "setting": {
      "speed": {
        
      }
    }
  }
}
```

### 例子
```json
1. mysql_to_hive示例
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "querySql": "", #自定义sql,支持多表关联，当用户配置querySql时，直接忽略table、column、where条件的配置。
                        "fetchSize": ""， #默认1024，该配置项定义了插件和数据库服务器端每次批量数据获取条数，该值决定了DataX和服务器端的网络交互次数，能够较大的提升数据抽取性能，注意，该值过大(>2048)可能造成DataX进程OOM
                        "splitPk": "db_id", #仅支持整形型数据切分；如果指定splitPk，表示用户希望使用splitPk代表的字段进行数据分片，如果该值为空，代表不切分，使用单通道进行抽取
                        "column": [], #"*"默认所有列,支持列裁剪，列换序
                        "connection": [
                            {
                                "jdbcUrl": ["jdbc:mysql://IP:3306/database?useUnicode=true&characterEncoding=utf8"],  #支持多数据源
                                "table": [] #支持多张表同时抽取
                            }
                        ],
                        "password": "",
                        "username": "",
                        "where": "" #指定的column、table、where条件拼接SQL，可以指定limit 10，也可以增量数据同步，如果该值为空，代表同步全表所有的信息
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [], #必须指定字段名，字段类型，{"name"："","tpye":""}
                        "compress": "", #hdfs文件压缩类型，默认不填写意味着没有压缩。其中：text类型文件支持压缩类型有gzip、bzip2;orc类型文件支持的压缩类型有NONE、SNAPPY（需要用户安装SnappyCodec）。 
                        "defaultFS": "", #Hadoop hdfs文件系统namenode节点地址。
                        "fieldDelimiter": "", #需要用户保证与创建的Hive表的字段分隔符一致
                        "fileName": "", #HdfsWriter写入时的文件名,需要指定表中所有字段名和字段类型，其中：name指定字段名，type指定字段类型。 
                        "fileType": "", #目前只支持用户配置为”text”或”orc”
                        "path": "", #存储到Hadoop hdfs文件系统的路径信息,hive表在hdfs上的存储路径
                        "hadoopConfig": {} #hadoopConfig里可以配置与Hadoop相关的一些高级参数，比如HA的配置。 
                        "writeMode": "" #append，写入前不做任何处理，文件名不冲突;nonConflict，如果目录下有fileName前缀的文件，直接报错。 
                    }
                }
            }
        ],
        "setting": {
            "speed": { #流量控制
                "byte": 1048576, #控制传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它
                "channel": ""  #控制同步时的并发数
                    }
            "errorLimit": { #脏数据控制
                "record": 0 #对脏数据最大记录数阈值（record值）或者脏数据占比阈值（percentage值，当数量或百分比，DataX Job报错退出
            }
        }
    }
}
```


```json
2. hive_to_mysql示例
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "hdfsreader",
                    "parameter": {
                        "column": [], #"*"默认所有列,指定Column信息时，type必须填写，index/value必须选择其一。 
                        "defaultFS": "", #hdfs文件系统namenode节点地址
                        "encoding": "UTF-8", #默认UTF-8
                        "nullFormat": "", #文本文件中无法使用标准字符串定义null(空指针),例如:nullFormat:”\N”，那么如果源头数据是”\N”
                        "compress": "", #orc文件类型下无需填写
                        "hadoopConfig": {}, #hadoopConfig里可以配置与Hadoop相关的一些高级参数，比如HA的配置。 
                        "fieldDelimiter": ",", #默认",";读取textfile数据时，需要指定字段分割符，HdfsReader在读取orcfile时，用户无需指定字段分割符
                        "fileType": "orc", #文件的类型，目前只支持用户配置为”text”、”orc”、”rc”、”seq”、”csv”。 
                        "path": "" #文件路径,支持多文件读取，可以使用"*"，也可以指定通配符遍历多文件,单文件只能单线程，多文件可以多线程，线程并发数通过通道数指定
                    }
                },
                "writer": {
                    "name": "mysqlwriter",
                    "parameter": {
                        "column": [], #必须指定，不能留空；如果要依次写入全部列，使用表示, 例如: "column": [""]，强烈不建议
                        "batchSize": "", #默认值1024 一次性批量提交的记录数大小，该值可以极大减少DataX与Mysql的网络交互次数，并提升整体吞吐量。但是该值设置过大可能会造成DataX运行进程OOM情况。
                        "connection": [
                            {
                                "jdbcUrl": "jdbc:mysql://IP:3306/database?useUnicode=true&characterEncoding=utf8",
                                "table": [] #支持写入一个或者多个表。当配置为多张表时，必须确保所有表结构保持一致。
                            }
                        ],
                        "password": "",
                        "preSql": [], #写入数据到目的表前，会先执行这里的标准语句。例在导入表前先进行删除操作：["delete from 表名"]
                        "postSql":[], #写入数据到目的表后，会执行这里的标准语句。（原理同 preSql ）
                        "session": [], #DataX在获取Mysql连接时，执行session指定的SQL语句，修改当前connection session属性
                        "username": "",
                        "writeMode": "" #默认insert ，可选insert/replace/update 
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": ""
            }
            "errorLimit": { #脏数据控制
                "record": 0 #对脏数据最大记录数阈值（record值）或者脏数据占比阈值（percentage值，当数量或百分比，DataX Job报错退出
            }
        }
    }
}
```
mysql2mysql
```json


{
  "job": {
    "setting": {
      "speed": {
        "channel": 3,
        "byte": 1048576
      },
      "errorLimit": {
        "record": 0,
        "percentage": 0.02
      }
    },
    "content": [
      {
        "reader": {
          "name": "mysqlreader",
          "parameter": {
            "username": "disabled",
            "password": "disabled",
            "splitPk": "",
            "column": [
              "aaa001",
              "aaa002",
              "aaa005",
              "aaa104",
              "aaa105",
              "prseno"
            ],
            "connection": [
              {
                "jdbcUrl": [
                  "jdbc:mysql://192.168.179.125:3306/disabled?useSSL=false&serverTimezone=GMT%2B8&allowPublicKeyRetrieval=true&useAffectedRows=true&zeroDateTimeBehavior=convertToNull"
                ],
                "table": [
                  "aa01"
                ]
              }
            ],
            "where": ""
          }
        },
        "writer": {
          "name": "mysqlwriter",
          "parameter": {
            "username": "root",
            "password": "root123",
            "column": [
              "aaa001",
              "aaa002",
              "aaa005",
              "aaa104",
              "aaa105",
              "prseno"
            ],
            "connection": [
              {
                "table": [
                  "aa01"
                ],
                "jdbcUrl": "jdbc:mysql://192.168.179.125:3306/mtms?useSSL=false&serverTimezone=GMT%2B8&allowPublicKeyRetrieval=true&useAffectedRows=true&zeroDateTimeBehavior=convertToNull"
              }
            ],
            "writeMode": "update",
            "preSql": []
          }
        }
      }
    ]
  }
}

```